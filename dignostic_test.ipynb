{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623ccb3-b0d7-4790-9391-f941a0c27132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size in memory 184, Type of object: <class 'list'>, Items: ['', '1', '23', '243', '33', '322', '24253', '5334', '3434', '34534']\n",
      "Size in memory 120, Type of object: <class 'tuple'>, Items: ('', '1', '23', '243', '33', '322', '24253', '5334', '3434', '34534')\n",
      "Size in memory 728, Type of object: <class 'set'>, Items: {'', '322', '3434', '23', '34534', '5334', '33', '243', '24253', '1'}\n",
      "Size in memory 352, Type of object: <class 'dict'>, Items: {0: '', 1: '1', 2: '23', 3: '243', 4: '33', 5: '322', 6: '24253', 7: '5334', 8: '3434', 9: '34534'}\n",
      "[23, '1', '23', '243', '33', '322', '24253', '5334', '3434', '34534']\n",
      "{'', '322', '3434', '23', '34534', '5334', 23, '33', '243', '24253', '1'}\n",
      "('', '1', '23', '243', '33', '322', '24253', '5334', '3434', '34534')\n",
      "{0: 23, 1: '1', 2: '23', 3: '243', 4: '33', 5: '322', 6: '24253', 7: '5334', 8: '3434', 9: '34534'}\n",
      "-2596062024600688771\n"
     ]
    }
   ],
   "source": [
    "# Q1 - List vs Tuple vs Set vs Dict\n",
    "# Write a function compare_collections() that:\n",
    "# a. Takes a list of 10 items as input. b. Converts it into tuple, set, and dictionary. c. Returns size in memory (sys.getsizeof) and type of each collection.\n",
    "# Print observations on mutability, ordering, and hashing.\n",
    "# Time Taken- 1hr\n",
    "import sys\n",
    "\n",
    "def compare_collections(lists):\n",
    "    # print(args) # why does it prints (['1', '2'],), I expected [('1', '2')]\n",
    "    # lists = list(*args)\n",
    "\n",
    "    tuples = tuple(lists)\n",
    "    sets = set(lists)\n",
    "    dicts = {}\n",
    "\n",
    "    for x in range(len(lists)):\n",
    "        dicts[x] = lists[x]\n",
    "\n",
    "    print(f'Size in memory {sys.getsizeof(lists)}, Type of object: {type(lists)}, Items: {lists}') # 184 bytes\n",
    "    print(f'Size in memory {sys.getsizeof(tuples)}, Type of object: {type(tuples)}, Items: {tuples}') # 120 bytes\n",
    "    print(f'Size in memory {sys.getsizeof(sets)}, Type of object: {type(sets)}, Items: {sets}') # 728 bytes\n",
    "    print(f'Size in memory {sys.getsizeof(dicts)}, Type of object: {type(dicts)}, Items: {dicts}') # 352 bytes\n",
    "\n",
    "    # Mutability, ordering and hashing\n",
    "\n",
    "    lists[0] = 23 # mutable\n",
    "    sets.add(23) # mutable, cannot use list operator though\n",
    "    # tuples[0] = 23 # immutable error -> TypeError: 'tuple' object does not support item assignment\n",
    "    dicts[0] = 23 # mutable1\n",
    "\n",
    "\n",
    "    print(lists) # ordered\n",
    "    print(sets) # unorderd -> random, unique elements\n",
    "    print(tuples) # ordered\n",
    "    print(dicts) # ordered -> always unique keys\n",
    "\n",
    "\n",
    "    # print(hash(lists)) # unhashable returns TypeError: unhashable type: 'list'\n",
    "    # print(hash(sets)) # unhashable returns TypeError: unhashable type: 'set'\n",
    "    print(hash(tuples)) # hashable returns hash number -278315504148916073\n",
    "    # print(hash(dicts)) # unhashable returns TypeError: unhashable type: 'dict'\n",
    "\n",
    "    # Question: why is set unhashable? They do contain only unique elements which is required for hashing, right? or is it something related to mutability rather than uniqueness?\n",
    "\n",
    "lists = []\n",
    "for i in range(10):\n",
    "    el  = input(f\"Enter List Element number {i+1}\")\n",
    "    lists.append(el)\n",
    "\n",
    "compare_collections(lists)\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79277a-8a34-49f6-9704-d25ebaa77661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\n",
      "[10, 20]\n",
      "[10]\n",
      "[10, 20]\n"
     ]
    }
   ],
   "source": [
    "# Q2: Function Behavior\n",
    "# What will be the output? Why?\n",
    "# Rewrite this function to avoid the issue.\n",
    "# Time Taken- 7mins\n",
    "\n",
    "def append_val(val, lst = []):\n",
    "    lst.append(val)\n",
    "    return lst\n",
    "\n",
    "print(append_val(10)) # [10]\n",
    "print(append_val(20)) # [20] 10 will not be appended because lst in in function scope so everytime it gets created when function is invoked and loses it memory when function completes\n",
    "\n",
    "# Modified\n",
    "lst = []\n",
    "def append_val(val):\n",
    "    global lst\n",
    "    lst.append(val)\n",
    "    return lst\n",
    "\n",
    "print(append_val(10)) # prints [10]\n",
    "print(append_val(20)) # prints [10, 20]\n",
    "\n",
    "######################################################################################\n",
    "# SECTION 2: OOP AND ERROR HANDLING (30MINS)\n",
    "\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e385c-90e8-4c03-af4a-03362441a9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Parsing Failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ex1.csv', 'ex2.csv', 'user_activity.csv']\n"
     ]
    }
   ],
   "source": [
    "# Q3: Build a Data Processor Class\n",
    "# Create a class BatchFileProcessor that:\n",
    "# Accepts a folder path, Lists all .csv files in that folder, Reads each file into a dictionary of DataFrames, Logs errors using the logging module (file not found, parse errors, etc.)\n",
    "# Bonus: Add a retry mechanism (max 3 attempts) for any failed file read.\n",
    "# Time Taken- 40mins\n",
    "import os\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "class BatchFileProcessor:\n",
    "    # csv_files: str = []\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "    def listing_files(self):\n",
    "        files = os.listdir(self.path)\n",
    "        csv_files = []\n",
    "        dict_files = {}\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                csv_files.append(file)\n",
    "        print(csv_files)\n",
    "        try:\n",
    "            for file in csv_files:\n",
    "                with open(file) as f:\n",
    "                    csv_reader = csv.DictReader(f)\n",
    "                    # was not able to dissect the meaning of- to read each file into a dictionary of dataframe\n",
    "                    for index, line in csv_reader:\n",
    "                        dict_files[index] = line\n",
    "                    print(dict_files)\n",
    "        except:\n",
    "            logging.error('Parsing Failed')\n",
    "        # source_dir = pathlib.Path(self.path)\n",
    "        # files = source_dir.iterdir\n",
    "        # for file in files:\n",
    "        #     print(file)\n",
    "\n",
    "path = os.getcwd()\n",
    "obj1 = BatchFileProcessor(path)\n",
    "obj1.listing_files()\n",
    "\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b98db-05c4-4af0-ad27-ff86409eefcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls Data validation failed!\n"
     ]
    }
   ],
   "source": [
    "# Q4. Custom Exception\n",
    "# Create a custom exception class DataValidationError. Use it in a function that: a. Validates that a DataFrame column has no nulls. b. Raises the error with a custom message if nulls are found.\n",
    "# Time Taken- 30mins\n",
    "import pandas as pd\n",
    "\n",
    "class DataValidationError(Exception):\n",
    "    def __init__(self, message=\"Nulls Data validation failed!\"):\n",
    "        super().__init__(message)\n",
    "        self.message = message\n",
    "        \n",
    "\n",
    "path = 'ex2.csv'\n",
    "column_name = 'name'\n",
    "try:\n",
    "    df = pd.read_csv(path)\n",
    "    if df[column_name].isnull().sum():\n",
    "        raise DataValidationError()\n",
    "    else:\n",
    "        print('Nulls Data validation passed!')\n",
    "    \n",
    "except DataValidationError as error:\n",
    "    print(error.message)\n",
    "\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986c7cc-1308-48d0-9e74-b0ba7122739c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls Data validation failed!\n"
     ]
    }
   ],
   "source": [
    "# Q4. Custom Exception\n",
    "# Create a custom exception class DataValidationError. Use it in a function that: a. Validates that a DataFrame column has no nulls. b. Raises the error with a custom message if nulls are found.\n",
    "# Time Taken- 30mins\n",
    "import pandas as pd\n",
    "\n",
    "class DataValidationError(Exception):\n",
    "    def __init__(self, message=\"Nulls Data validation failed!\"):\n",
    "        super().__init__(message)\n",
    "        self.message = message\n",
    "        \n",
    "\n",
    "path = 'ex2.csv'\n",
    "column_name = 'name'\n",
    "try:\n",
    "    df = pd.read_csv(path)\n",
    "    if df[column_name].isnull().sum():\n",
    "        raise DataValidationError()\n",
    "    else:\n",
    "        print('Nulls Data validation passed!')\n",
    "    \n",
    "except DataValidationError as error:\n",
    "    print(error.message)\n",
    "\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7863c15-7892-469d-a74d-c3aa5066b9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error message: division by zero\n",
      "Retrying\n",
      "Error message: division by zero\n",
      "Retrying\n",
      "Error message: division by zero\n",
      "Failed 3 times\n"
     ]
    }
   ],
   "source": [
    "# SECTION 3: DECORATORS, GENERATORS, CONTEXT MANAGERS\n",
    "\n",
    "# Q5. Retry Decorator\n",
    "# Write a decorator @retry_on_fail that:\n",
    "# Retries a function up to 3 times, Waits 1 second between retries (use time.sleep), Catches all exceptions and logs failure reason\n",
    "# Time Taken- 20mins\n",
    "import time\n",
    "def retry_on_fail(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        for i in range(3):     \n",
    "            try:\n",
    "                func(*args, **kwargs)\n",
    "                break\n",
    "            except Exception as err:\n",
    "                time.sleep(1)\n",
    "                if i == 2:\n",
    "                    print(f'Error message: {err}')\n",
    "                    print('Failed 3 times') \n",
    "                else:\n",
    "                    print(f'Error message: {err}')\n",
    "                    print('Retrying')\n",
    "    return wrapper\n",
    "\n",
    "@retry_on_fail\n",
    "def divide(a, b):\n",
    "    print(a/b)\n",
    "\n",
    "divide(1,0)\n",
    "\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e527548-b143-49dc-9ca6-07be7933d0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Generator Pipeline\n",
    "# Implement a 3 stage generator pipeline:\n",
    "# 1. read_lines(file_path) - yields each line from a file\n",
    "# 2. filter_keywords(lines, keyword) - yields only lines with a keyword\n",
    "# 3. parse_to_dict(lines) - yields dicts assuming each line is comma-seperated: name, age, city\n",
    "# Time Taken- 10mins\n",
    "\n",
    "def read_lines(file_path):\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            yield f.readline()\n",
    "\n",
    "def filter_keywords(lines, keyword):\n",
    "    if keyword in lines:\n",
    "        yield lines\n",
    "\n",
    "dicts = {}\n",
    "\n",
    "def parse_to_dict(lines):\n",
    "    for index, word in enumerate(lines):\n",
    "        dict[index] = word \n",
    "\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b69955-3dad-489b-aa9a-069d334418e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Spent in this function: 2\n"
     ]
    }
   ],
   "source": [
    "# Q7. Context Manager\n",
    "# create a context manager Timer() that:\n",
    "# a. on enter, notes the current time, b. on exit, prints total elapsed time in seconds.\n",
    "# Use it around a time-consuming loop to show output\n",
    "# Time Taken- 8mins\n",
    "import time\n",
    "\n",
    "def Timer():\n",
    "    curr_time = time.time()\n",
    "    time.sleep(2)\n",
    "    print(f'Time Spent in this function: {round(abs(time.time()-curr_time))}')\n",
    "\n",
    "Timer()\n",
    "\n",
    "#############################################################################\n",
    "# SECTION 4: DATA MANIPULATION \n",
    "\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aceeed-658f-4c4e-8aff-33966fc95b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  event           timestamp  hour\n",
      "0        1  login 2024-01-01 09:00:00     9\n",
      "2        2  login 2024-01-01 09:30:00     9\n"
     ]
    }
   ],
   "source": [
    "# Q8: Pandas + Transformation\n",
    "# from a user activity csv file, write a script that:\n",
    "# a. Reads the file into a dataframe b. converts 'timestamp' into datetime c. Filters only login events d. adds a column hour extracted from timestamp\n",
    "# Time Taken- 10mins\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path = 'user_activity.csv'\n",
    "df = pd.read_csv(path)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df = df[df['event'] == 'login']\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "print(df)\n",
    "\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81237145-639a-45b2-a4eb-5fec335db4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[           0            1            4 ... 999994000009 999996000004\n",
      " 999998000001]\n",
      "loop total time: 1.374664068222046\n",
      "[           0            1            4 ... 999994000009 999996000004\n",
      " 999998000001]\n",
      "List Comprehension total time: 0.8623137474060059\n",
      "[           0            1            4 ... 999994000009 999996000004\n",
      " 999998000001]\n",
      "Vectorization total time: 0.013051986694335938\n"
     ]
    }
   ],
   "source": [
    "# Q9. Numpy Performance Check\n",
    "# create a list of 1 miilion numbers. Convert it into a numpy array. Calculate the square of each number using a. Python Loop b. List Comprehension c. Numpy Vectorization. Compare time takens for each.\n",
    "# Time Taken 30mins\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "lists = []\n",
    "for i in range(1_000_000):\n",
    "    lists.append(i)\n",
    "\n",
    "arr1 = np.array(lists)\n",
    "arr2 = np.array(lists)\n",
    "arr3 = np.array(lists)\n",
    "\n",
    "start_time = time.time()\n",
    "for ele in range(len(arr1)):\n",
    "    arr1[ele] = arr1[ele]*arr1[ele]\n",
    "end_time = time.time()\n",
    "loop_total = end_time - start_time\n",
    "print(arr1)\n",
    "print(f'loop total time: {loop_total}') # 1.554 sec\n",
    "\n",
    "start_time = time.time()\n",
    "arr2 = np.array([ele*ele for ele in arr2])\n",
    "end_time = time.time()\n",
    "print(arr2)\n",
    "comprehension_total = end_time - start_time\n",
    "print(f'List Comprehension total time: {comprehension_total}') # 0.524 sec\n",
    "\n",
    "start_time = time.time()\n",
    "arr3 = arr3*arr3\n",
    "end_time = time.time()\n",
    "print(arr3)\n",
    "vec_total = end_time - start_time\n",
    "print(f'Vectorization total time: {vec_total}') # 0.025 sec\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# SECTION 5: REAL WORLD ETL LOGIC\n",
    "\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146f3e9-ee0e-493b-ac6b-1572bd67a9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice Smith\n",
      "Bob Jones\n",
      "Carol White\n"
     ]
    }
   ],
   "source": [
    "# Q10 Parse JSON and load Mock\n",
    "# write a function parse_and_load(file_path) that: a. reads a JSON file with nested user data b. Flattens it into a dataframe c. Mocks a DB insert using a function db_insert(data: List[dict])\n",
    "import json\n",
    "file_path='users.json'\n",
    "def parse_and_load(file_path):\n",
    "    with open(file_path, mode='r') as f:\n",
    "        data = json.load(f)\n",
    "        for line in data:\n",
    "            print(line['name'])\n",
    "\n",
    "parse_and_load(file_path)\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b4ba0-f524-4942-99b2-5d91a89790c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for XML Files...\n",
      "Looking for XML Files...\n",
      "Looking for XML Files...\n"
     ]
    }
   ],
   "source": [
    "# Q11. Folder Watcher Simulation\n",
    "# Simulate a mini-folder watcher that: a. scans a folderevery 10 seconds b. identifies new '.xml' files c. Parses them using ElementTree d. moves successfully processed files to an archive folder \n",
    "\n",
    "import time \n",
    "import os\n",
    "import shutil\n",
    "\n",
    "start_time = time.time()\n",
    "xml_files = set()\n",
    "def folder_scanner(folder_path):\n",
    "    print('Looking for XML Files...')\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        if file.endswith('.xml') and file not in xml_files:\n",
    "            xml_files.add(file)\n",
    "            print(f'New Xml File detected- {file}')\n",
    "            print('To Do Parsing Logic')\n",
    "            if not os.path.exists('./archive'):\n",
    "                os.mkdir('./archive')\n",
    "            print(f'Source Path- {os.getcwd()}\\\\xml_files\\\\{file}')\n",
    "            shutil.move(f'{os.getcwd()}\\\\xml_files\\\\{file}', f'{os.getcwd()}\\\\archive')\n",
    "            print(f'Moved {file} to archive folder')\n",
    "\n",
    "path = './xml_files/'\n",
    "# retring 3 times after every 10 seconds\n",
    "for i in range(3):\n",
    "    folder_scanner(path)\n",
    "    time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
